{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T06:46:12.128988Z",
     "start_time": "2019-06-04T06:46:12.083380Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T07:05:44.227734Z",
     "start_time": "2019-06-03T07:05:44.206874Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_glove(dim, glove_dir):\n",
    "    \"\"\"Load glove vectors into a dictionary mapping word to vector.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    dim: int\n",
    "        Size of embedding. One of (50, 100, 200, 300).\n",
    "    glove_dir: str\n",
    "        Path to directory containing glove files.\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    Dictionary where keys are words and values are {dim}-dimensional ndarrays.\n",
    "    \"\"\"\n",
    "    w2vec = dict()\n",
    "    path = os.path.join(glove_dir, f'glove.6B.{dim}d.txt')\n",
    "    with open(path, 'r') as f:\n",
    "        for row in f:\n",
    "            items = row.split()\n",
    "            w2vec[items[0]] = np.array(items[1:], dtype=float)\n",
    "    return w2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T07:05:44.925982Z",
     "start_time": "2019-06-03T07:05:44.917978Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_pickle(obj, fname, dir_name='data'):\n",
    "    \"\"\"Wrapper to quickly save a pickled object.\"\"\"\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "    path = os.path.join(dir_name, f'{fname}.pkl')\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "    print(f'Data written to {path}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T07:05:45.590197Z",
     "start_time": "2019-06-03T07:05:45.582825Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_pickle(fname, dir_name='data'):\n",
    "    \"\"\"Wrapper to quickly load a pickled object.\"\"\"\n",
    "    with open(os.path.join(dir_name, f'{fname}.pkl'), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T07:05:46.446998Z",
     "start_time": "2019-06-03T07:05:46.434820Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_val_test_split(x, y, train_p, val_p, state=1, shuffle=True):\n",
    "    \"\"\"Wrapper to split data into train, validation, and test sets.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    x: pd.DataFrame, np.ndarray\n",
    "        Features\n",
    "    y: pd.DataFrame, np.ndarray\n",
    "        Labels\n",
    "    train_p: float\n",
    "        Percent of data to assign to train set.\n",
    "    val_p: float\n",
    "        Percent of data to assign to validation set.\n",
    "    state: int or None\n",
    "        Int will make the split repeatable. None will give a different random\n",
    "        split each time.\n",
    "    shuffle: bool\n",
    "        If True, randomly shuffle the data before splitting.\n",
    "    \"\"\"\n",
    "    test_p = 1 - val_p/(1-train_p)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, \n",
    "                                                        y,\n",
    "                                                        train_size=train_p,\n",
    "                                                        shuffle=shuffle,\n",
    "                                                        random_state=state)\n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_test, \n",
    "                                                    y_test,\n",
    "                                                    test_size=test_p,\n",
    "                                                    random_state=state)\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T06:28:09.743960Z",
     "start_time": "2019-06-04T06:28:09.665760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7106744, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtypes = dict(text=object, sex='category', age=np.int8)\n",
    "df = pd.read_csv('data/sentences.csv', dtype=dtypes, usecols=dtypes.keys())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T07:06:58.198517Z",
     "start_time": "2019-06-03T07:06:58.178556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8730028 entries, 0 to 8730027\n",
      "Data columns (total 3 columns):\n",
      "text    object\n",
      "sex     category\n",
      "age     int8\n",
      "dtypes: category(1), int8(1), object(1)\n",
      "memory usage: 83.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T07:08:24.627064Z",
     "start_time": "2019-06-03T07:07:24.351449Z"
    }
   },
   "outputs": [],
   "source": [
    "lengths = df.text.str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T07:09:08.718486Z",
     "start_time": "2019-06-03T07:08:57.668668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00        1.0\n",
       "0.05        2.0\n",
       "0.10        3.0\n",
       "0.20        5.0\n",
       "0.50       12.0\n",
       "0.75       20.0\n",
       "0.90       29.0\n",
       "0.95       37.0\n",
       "0.98       51.0\n",
       "0.99       67.0\n",
       "1.00    10276.0\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.quantile([0, .05, .1, .2, .5, .75, .9, .95, .98, .99, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T07:10:41.187260Z",
     "start_time": "2019-06-03T07:10:37.043276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7106744, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[(lengths >= 5) & (lengths <= 50)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T07:11:39.424832Z",
     "start_time": "2019-06-03T07:11:39.417922Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7106744 entries, 0 to 8730027\n",
      "Data columns (total 3 columns):\n",
      "text    object\n",
      "sex     category\n",
      "age     int8\n",
      "dtypes: category(1), int8(1), object(1)\n",
      "memory usage: 122.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T06:48:52.142960Z",
     "start_time": "2019-06-04T06:48:48.662436Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note: tokenizer is faster but leaves punctuation attached to words.\n",
    "# NLTK tokenizer has issues with ellipses. Must disable parser, tagger, and \n",
    "# ner in nlp() when working with the whole dataset to avoid memory issues.\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 600_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T06:55:31.444772Z",
     "start_time": "2019-06-04T06:55:31.436629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6822474,) (6822474, 2)\n",
      "(142134,) (142134, 2)\n",
      "(142136,) (142136, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split data into train, validation, and test sets.\n",
    "data = train_val_test_split(df.text, df[['sex', 'age']], train_p=.96, \n",
    "                            val_p=.02, shuffle=True, state=1)\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = data\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T06:49:36.809217Z",
     "start_time": "2019-06-04T06:49:27.556801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to data/split_data.pkl.\n"
     ]
    }
   ],
   "source": [
    "save_pickle(data, 'split_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T06:51:48.821464Z",
     "start_time": "2019-06-04T06:51:48.796616Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_word_mappings(x_train, nlp, glove_dir):\n",
    "    \"\"\"Generate word to count, word to index, and word to vector mappings.\"\"\"\n",
    "    # Map each token to the # of times it appears in the corpus.\n",
    "    tokens = [item for t in nlp(' '.join(x_train.values),\n",
    "                                disable=['parser', 'tagger', 'ner'])\n",
    "              for item in [t.text.strip()] if item]\n",
    "    w2count = dict(filter(lambda x: x[1] > 4, Counter(tokens).items()))\n",
    "    save_pickle(w2count, 'w2count')\n",
    "\n",
    "    # Construct w2idx dict and i2w list.\n",
    "    w2idx = {k: i for i, (k, v) in\n",
    "             enumerate(sorted(w2count.items(), key=lambda x: x[1]), 2)}\n",
    "    w2idx['<PAD>'] = 0\n",
    "    w2idx['<UNK>'] = 1\n",
    "    i2w = [k for k, v in sorted(w2idx.items(), key=lambda x: x[1])]\n",
    "    save_pickle(w2idx, 'w2idx')\n",
    "    save_pickle(i2w, 'i2w')\n",
    "\n",
    "    # Load word vectors and filter to include words in our vocab.\n",
    "    w2vec = load_glove(300, glove_dir)\n",
    "    w2vec = {k: v for k, v in w2vec.items() if k in w2idx}\n",
    "    save_pickle(w2vec, 'w2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T07:56:12.719551Z",
     "start_time": "2019-06-03T07:12:53.190168Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = [item for t in nlp(' '.join(x_train.values), \n",
    "                            disable=['parser', 'tagger', 'ner'])\n",
    "          for item in [t.text.strip()] if item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T10:16:17.989892Z",
     "start_time": "2019-06-03T10:15:24.543388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190295"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2count = dict(filter(lambda x: x[1] > 4, Counter(tokens).items()))\n",
    "len(w2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T15:19:18.254172Z",
     "start_time": "2019-06-03T15:18:48.679649Z"
    }
   },
   "outputs": [],
   "source": [
    "w2idx = {k: i for i, (k, v) in \n",
    "         enumerate(sorted(w2count.items(), key=lambda x: x[1]), 2)}\n",
    "w2idx['<PAD>'] = 0\n",
    "w2idx['<UNK>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T15:19:18.430026Z",
     "start_time": "2019-06-03T15:19:18.265023Z"
    }
   },
   "outputs": [],
   "source": [
    "i2w = [k for k, v in sorted(w2idx.items(), key=lambda x: x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T15:21:34.663357Z",
     "start_time": "2019-06-03T15:20:48.210839Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73985"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dir = '/Users/hmamin/data/glove/'\n",
    "\n",
    "# Load word vectors and filter to include words in our vocab.\n",
    "w2vec = load_glove(300, glove_dir)\n",
    "w2vec = {k: v for k, v in w2vec.items() if k in w2idx}\n",
    "len(w2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T15:24:09.658126Z",
     "start_time": "2019-06-03T15:21:55.076423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to data/tokens.pkl.\n",
      "Data written to data/w2count.pkl.\n",
      "Data written to data/w2idx.pkl.\n",
      "Data written to data/w2vec.pkl.\n"
     ]
    }
   ],
   "source": [
    "save_pickle(tokens, 'tokens')\n",
    "save_pickle(w2count, 'w2count')\n",
    "save_pickle(w2idx, 'w2idx')\n",
    "save_pickle(w2vec, 'w2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-load word mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T15:37:42.975073Z",
     "start_time": "2019-06-03T15:37:42.950857Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode(text, w2idx, nlp):\n",
    "    \"\"\"Map each word in a post to its index in the embedding matrix. Posts\n",
    "    retain their original lengths for now.\n",
    "    \"\"\"\n",
    "    unk = w2idx['<UNK>']\n",
    "    return [w2idx.get(word.text, unk) \n",
    "            for word in nlp(text, disable=['parser', 'tagger', 'ner'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T15:40:27.343677Z",
     "start_time": "2019-06-03T15:37:43.820912Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = load_pickle('tokens')\n",
    "w2count = load_pickle('w2count')\n",
    "w2idx = load_pickle('w2idx')\n",
    "w2vec = load_pickle('w2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T15:42:01.268951Z",
     "start_time": "2019-06-03T15:42:01.059211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[179660,\n",
       " 190256,\n",
       " 190281,\n",
       " 190146,\n",
       " 189840,\n",
       " 91279,\n",
       " 190254,\n",
       " 190290,\n",
       " 181001,\n",
       " 187235,\n",
       " 190296]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(x_train.values[0], w2idx, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T15:47:16.287322Z",
     "start_time": "2019-06-03T15:46:30.450121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.389\n",
      "0.867\n"
     ]
    }
   ],
   "source": [
    "# Only have pre-trained vectors for 38.9% of the unique vocab for our corpus.\n",
    "# However, this makes up 86.7% of the total words in our corpus.\n",
    "pretrained_pct_unique = np.mean([w1 in w2vec for w1 in w2idx])\n",
    "pretrained_pct_total = np.mean([t in w2vec for t in tokens])\n",
    "\n",
    "print(round(pretrained_pct_unique, 3))\n",
    "print(round(pretrained_pct_total, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Think about how to deal with words in w2idx but not w2vec; can just ignore, but not ideal. Maybe init to zero in embedding matrix and make emb trainable, but zero the grads for the pre-trained vectors (see stackoverflow: https://stackoverflow.com/questions/54924582/is-it-possible-to-freeze-only-certain-embedding-weights-in-the-embedding-layer-i/54952825#54952825)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
