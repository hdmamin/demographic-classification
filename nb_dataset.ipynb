{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T03:58:37.290324Z",
     "start_time": "2019-06-05T03:58:37.285322Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import gc\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils import save_pickle, load_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T04:38:46.111980Z",
     "start_time": "2019-06-05T04:38:46.100279Z"
    }
   },
   "outputs": [],
   "source": [
    "class BlogDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y, vocab_size, cbow=False, max_len=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        -----------\n",
    "        x: pd.Series\n",
    "            One of the outputs containing features from our \n",
    "            train_val_test_split().\n",
    "        y: pd.DataFrame\n",
    "            One of the outputs containing labels from our \n",
    "            train_val_test_split(). The first column contains gender \n",
    "            (0=female, 1=male) while the second column contains age.\n",
    "        vocab_size: int\n",
    "            Number of words in our vocabulary. We use w2idx for this, not \n",
    "            w2vec, since we may want to train embeddings for the words not\n",
    "            in the GloVe vectors.\n",
    "        cbow: bool\n",
    "            If True, this assumes the dataset will be used with a CBOW model\n",
    "            where word order doesn't matter. It will encode the x vector to \n",
    "            contain counts of word appearances in the sentence but will not\n",
    "            retain anything about the temporal structure of the data. If \n",
    "            False, we return x as a vector of glove indices in the order the\n",
    "            words appear in the sentence.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y.values.astype(np.float32)\n",
    "        self.cbow = cbow\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"The CBOW model is just used as a simple baseline for gender \n",
    "        classification, so only the first column of y is needed in that case.\n",
    "        \"\"\"\n",
    "        x, y = encode(self.x[i], w2idx, nlp), self.y[i]\n",
    "        if self.cbow:\n",
    "            x = count_encode(x, self.vocab_size)\n",
    "            y = y[0]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.max_len:\n",
    "            return self.max_len\n",
    "        return self.x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T04:38:47.067131Z",
     "start_time": "2019-06-05T04:38:47.060232Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode(text, w2idx, nlp):\n",
    "    \"\"\"Map each word in a post to its index in the embedding matrix. Posts\n",
    "    retain their original lengths for now.\n",
    "    \"\"\"\n",
    "    unk = w2idx['<UNK>']\n",
    "    return [w2idx.get(word.text, unk) \n",
    "            for word in nlp(text, disable=['parser', 'tagger', 'ner'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T04:38:47.675034Z",
     "start_time": "2019-06-05T04:38:47.668520Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_encode(idx_encoded, vocab_size):\n",
    "    \"\"\"Convert a vector of glove indices into a vector with the same length\n",
    "    as our vocabulary. Each number will contain the count of the times that\n",
    "    the corresponding word appeared in the original sentence. \n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    idx_encoded: np.ndarray\n",
    "        Array of glove indices for a single sentence, with one number for each\n",
    "        word.\n",
    "    vocab_size: int\n",
    "        Number of words in our vocabulary.\n",
    "    \"\"\"\n",
    "    vec = np.zeros(vocab_size)\n",
    "    for idx in idx_encoded:\n",
    "        vec[idx] += 1\n",
    "    return vec.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T19:06:51.120517Z",
     "start_time": "2019-06-04T19:06:45.598002Z"
    }
   },
   "outputs": [],
   "source": [
    "w2count = load_pickle('w2count')\n",
    "w2idx = load_pickle('w2idx')\n",
    "w2vec = load_pickle('w2vec')\n",
    "i2w = load_pickle('i2w')\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = load_pickle('split_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T04:04:16.882635Z",
     "start_time": "2019-06-05T04:04:16.878467Z"
    }
   },
   "outputs": [],
   "source": [
    "bs = 64\n",
    "workers = 2\n",
    "ngpu = 1\n",
    "vocab_size = len(w2idx)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() and ngpu > 0 \n",
    "                      else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T07:19:49.701777Z",
     "start_time": "2019-06-04T07:19:48.491333Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T03:47:53.275840Z",
     "start_time": "2019-06-05T03:47:53.200037Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def dims(self):\n",
    "        \"\"\"Get shape of each layer's weights.\"\"\"\n",
    "        return [p.shape for p in self.parameters()]\n",
    "    \n",
    "    def trainable(self):\n",
    "        \"\"\"Check which layers are trainable.\"\"\"\n",
    "        return [p.requires_grad for p in self.parameters()]\n",
    "    \n",
    "    def layer_stats(self):\n",
    "        \"\"\"Check mean and standard deviation of each layer's weights.\"\"\"\n",
    "        return [(round(p.data.mean().item(), 3), \n",
    "                 round(p.data.std().item(), 3))\n",
    "                 for p in self.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T03:48:04.755366Z",
     "start_time": "2019-06-05T03:48:04.738827Z"
    }
   },
   "outputs": [],
   "source": [
    "class GenderCBow(BaseModel):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T03:48:33.220236Z",
     "start_time": "2019-06-05T03:48:32.990922Z"
    }
   },
   "outputs": [],
   "source": [
    "gender_model = GenderCBow(len(w2idx), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T06:01:56.811544Z",
     "start_time": "2019-06-05T06:01:56.806436Z"
    }
   },
   "outputs": [],
   "source": [
    "def correct(y_hat, y, reduction, sigmoid=True):\n",
    "    \"\"\"Calculate the accuracy if reduction='mean' or the total correct if \n",
    "    reduction='sum'.\n",
    "    \n",
    "    y_hat: torch.tensor\n",
    "        2D tensor of predictions.\n",
    "    y: torch.tensor\n",
    "        2D tensor of labels.\n",
    "    reduction: str\n",
    "        'sum' or 'mean'\n",
    "    sigmoid: bool\n",
    "        If True, apply sigmoid before calculating number correct.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        if sigmoid:\n",
    "            y_hat = torch.sigmoid(y_hat)\n",
    "        correct = torch.round(y_hat).eq(y).float()\n",
    "        if reduction == 'sum':\n",
    "            return torch.sum(correct).item()\n",
    "        elif reduction == 'mean':\n",
    "            return torch.mean(correct).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T06:58:20.302459Z",
     "start_time": "2019-06-05T06:58:20.295032Z"
    }
   },
   "outputs": [],
   "source": [
    "def val_metrics(model, val_dl):\n",
    "    \"\"\"Return validation loss and accuracy.\"\"\"\n",
    "    stats = defaultdict(int)\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_dl:\n",
    "            x = x.to(device)\n",
    "            y = y.unsqueeze(-1).to(device)\n",
    "            batch_size = x.shape[0]\n",
    "            y_hat = model(x)\n",
    "            loss = F.binary_cross_entropy_with_logits(y_hat, y, \n",
    "                                                      reduction='sum')\n",
    "            \n",
    "            # Update stats at end of batch.\n",
    "            stats['loss'] += loss\n",
    "            stats['total'] += batch_size\n",
    "            stats['correct'] += correct(y_hat, y, 'sum', sigmoid=True)\n",
    "\n",
    "    return stats['loss'] / stats['total'], stats['correct'] / stats['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T06:58:29.359812Z",
     "start_time": "2019-06-05T06:58:29.348156Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epochs, train_dl, val_dl, lr=3e-3, model=None, optim=None, dim=100):\n",
    "    if not model:\n",
    "        model = GenderCBow(vocab_size, dim)\n",
    "        optim = torch.optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    # Store stats to return at end.\n",
    "    stats = defaultdict(list)\n",
    "    \n",
    "    # Begin training loop.\n",
    "    for epoch in range(epochs):\n",
    "        e_stats = defaultdict(int)\n",
    "        model.train()\n",
    "        \n",
    "        # Loop through mini batches.\n",
    "        for i, (x, y) in enumerate(train_dl):\n",
    "            x = x.to(device)\n",
    "            y = y.unsqueeze(-1).to(device)\n",
    "            batch_size = x.shape[0]\n",
    "            \n",
    "            # Forward and backward pass.\n",
    "            optim.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            loss = F.binary_cross_entropy_with_logits(y_hat, y, \n",
    "                                                      reduction='mean')\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            # Update stats at end of batch.\n",
    "            e_stats['train_loss'] += loss * batch_size\n",
    "            e_stats['total'] += batch_size\n",
    "            e_stats['correct'] += correct(y_hat, y, reduction='sum')\n",
    "            \n",
    "        # Update stats at end of epoch.\n",
    "        e_stats['val_loss'], e_stats['val_acc'] = val_metrics(model, val_dl)\n",
    "        e_stats['train_acc'] = e_stats['correct'] / e_stats['total']\n",
    "        e_stats['train_loss'] = e_stats['train_loss'] / e_stats['total']\n",
    "        \n",
    "        # Update stats at end of epoch.   \n",
    "        for key in ('train_loss', 'train_acc', 'val_loss', 'val_acc'):\n",
    "            stats[key].append(e_stats[key])\n",
    "\n",
    "        print(f\"\\nEpoch [{epoch+1}/{epochs}]\"\n",
    "              f\"\\nTrain loss: {e_stats['train_loss']:.3f}\"\n",
    "              f\"\\tTrain acc: {e_stats['train_acc']:.3f}\"\n",
    "              f\"\\nValidation loss: {e_stats['val_loss']:.3f}\"\n",
    "              f\"\\tValidation acc: {e_stats['val_acc']:.3f}\"\n",
    "             )\n",
    "\n",
    "    return dict(model=model, optim=optim, stats=stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:04:19.552159Z",
     "start_time": "2019-06-05T07:04:19.189887Z"
    }
   },
   "outputs": [],
   "source": [
    "ds_train = BlogDataset(x_train, y_train, len(w2idx), cbow=True, max_len=1_000)\n",
    "ds_val = BlogDataset(x_train, y_train, len(w2idx), cbow=True, max_len=5_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:04:20.048739Z",
     "start_time": "2019-06-05T07:04:20.042724Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(ds_train, batch_size=bs, shuffle=True, \n",
    "                      num_workers=workers)\n",
    "val_dl = DataLoader(ds_val, batch_size=bs, shuffle=False, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:04:21.625229Z",
     "start_time": "2019-06-05T07:04:20.981486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 190297]), torch.Size([64]))"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, y1 = next(iter(train_dl))\n",
    "x1.shape, y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T07:04:58.471496Z",
     "start_time": "2019-06-05T07:04:26.608447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/3]\n",
      "Train loss: 0.692\tTrain acc: 0.511\n",
      "Validation loss: 0.681\tValidation acc: 0.585\n",
      "\n",
      "Epoch [2/3]\n",
      "Train loss: 0.598\tTrain acc: 0.808\n",
      "Validation loss: 0.658\tValidation acc: 0.602\n",
      "\n",
      "Epoch [3/3]\n",
      "Train loss: 0.428\tTrain acc: 0.866\n",
      "Validation loss: 0.680\tValidation acc: 0.616\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "output = train(epochs, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T05:41:10.402369Z",
     "start_time": "2019-06-05T05:41:10.367653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct(output['model'](x1), y1, 'sum', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T05:27:48.912525Z",
     "start_time": "2019-06-05T05:27:48.864985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(output['model'](x1))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-05T05:27:52.380715Z",
     "start_time": "2019-06-05T05:27:52.371874Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
